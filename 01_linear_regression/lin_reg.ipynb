{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vectors are in bold lower case letters: $\\bm{x}$\n",
    "- Matrix \n",
    "- Vector components are in a subscript $\\bm{x}_2$\n",
    "- Data element number is in parenthesis in a superscript: $\\bm{x}^{(3)}$\n",
    "- $x$ is input/independent variable/features\n",
    "- $y$ is output/dependent variable/answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the notation with some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "example_data = pd.DataFrame()\n",
    "names = ['Neil', 'Buzz', 'Michael']\n",
    "experience = [20, 22, 23]\n",
    "education = [16, 21, 18]\n",
    "example_data['name'] = names\n",
    "example_data['experience_years'] = experience\n",
    "example_data['education_years'] = education\n",
    "example_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think salary is linearly correlated to years of experience and years of education, the regression equation is:\n",
    "\n",
    "$$\\hat{y}^{(i)} = b + m_1 x_1^{(i)} + m_2 x_2^{(i)}$$\n",
    "\n",
    "- $x_1$ is years of experience\n",
    "- $m_1$ is the coefficient for $x_1$ that we need to solve for/learn\n",
    "- $x_2$ is years of education\n",
    "- $m_2$ is the coefficient for $x_2$ that we need to solve for/learn\n",
    "- $b$ is the y-intercept that we need to solve for/learn\n",
    "\n",
    "For Buzz's predicted salary:\n",
    "$$\\hat{y}^{(1)} = b + m_1 x_1^{(1)} + m_2 x_2^{(1)}$$\n",
    "$$\\hat{y}^{(1)} = b + m_1 (22) + m_2 (21)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write this in matrix notation as well (which is more convenient to write and code when dealing with a lot of inputs):\n",
    "\n",
    "$$ \\hat{\\bm{y}} = \\bm{Xw} $$\n",
    "\n",
    "- $\\bm{X}$ is the input Matrix:\n",
    "\n",
    "$$\\bm{X} = \\left[\\begin{array}{cc}\n",
    "    1 && x_1^{(0)} && x_2^{(0)}\\\\\n",
    "    1 && x_1^{(1)} && x_2^{(1)}\\\\\n",
    "    1 && x_1^{(2)} && x_2^{(2)}\\\\\n",
    "   \\end{array}\\right]$$\n",
    "\n",
    "- $\\bm{w}$ is the parameters of linear regression to solve for/learn:\n",
    "\n",
    "$$\\bm{w} = \\left[\\begin{array}{cc}\n",
    "    b\\\\\n",
    "    m_1\\\\\n",
    "    m_2\\\\\n",
    "   \\end{array}\\right]$$\n",
    "\n",
    "- $\\hat{\\bm{y}}$ is the output vector:\n",
    "\n",
    "$$\\hat{\\bm{y}} = \\left[\\begin{array}{cc}\n",
    "    \\hat{y}^{(0)} \\\\\n",
    "    \\hat{y}^{(1)} \\\\\n",
    "    \\hat{y}^{(2)} \\\\\n",
    "   \\end{array}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of linear regression is to find a line that best fits data points.\n",
    "\n",
    "To keep it simple, we'll first do a one input, $x$, and one output, $y$, regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.normal(torch.zeros(5),torch.ones(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n",
    "Y = X+torch.normal(torch.zeros(5),torch.ones(5))\n",
    "line = deepcopy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.update_layout( height=500, width=750, margin=dict(l=20, r=20, t=20, b=20), xaxis=dict(title='X'), yaxis=dict(title='Y'))\n",
    "data_trace = go.Scatter( x=X, y=Y, mode='markers', name='data', marker=dict(size=8, color=px.colors.qualitative.Plotly[2]) )\n",
    "fig.add_traces(data_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to fit a line to the points above. Maybe you draw something that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.update_layout( height=500, width=750, margin=dict(l=20, r=20, t=20, b=20), xaxis=dict(title='X'), yaxis=dict(title='Y'))\n",
    "data_trace = go.Scatter( x=X, y=Y, mode='markers', name='data', marker=dict(size=8, color=px.colors.qualitative.Plotly[2]))\n",
    "line_trace = go.Scatter( x=X, y=line, mode='lines', name='regression line', line=dict(color=px.colors.qualitative.Plotly[0]))\n",
    "fig.add_traces([data_trace, line_trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors/Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well or bad we did, we need to subtract our predicted values, $\\bm{\\hat{y}}$, (the dots on the regression line) with the actual values, $\\bm{y}$.\n",
    "\n",
    "This gives us the residuals or errors, $\\bm{e}$:\n",
    "$$\\bm{e} = \\bm{\\hat{y}} - \\bm{y}$$\n",
    "$$\\left[\\begin{array}{cc}\n",
    "    e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_N \n",
    "   \\end{array}\\right]\n",
    "   =\n",
    "   \\left[\\begin{array}{cc}\n",
    "    \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_N \n",
    "   \\end{array}\\right]\n",
    "   -\n",
    "   \\left[\\begin{array}{cc}\n",
    "    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \n",
    "   \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.update_layout( height=500, width=750, margin=dict(l=20, r=20, t=20, b=20), xaxis=dict(title='X'), yaxis=dict(title='Y'))\n",
    "traces = []\n",
    "for i, x in enumerate(line):\n",
    "    if i>0:\n",
    "        showlegend = False\n",
    "    else:\n",
    "        showlegend = True\n",
    "    residual_trace = go.Scatter( x=[x, x], y=[line[i], Y[i]], line=dict(color=px.colors.qualitative.Plotly[1], dash='dash'), mode='lines', name='residuals/errors', showlegend=showlegend)\n",
    "    traces.append(residual_trace)\n",
    "data_trace = go.Scatter( x=X, y=Y, mode='markers', name='data', marker=dict(size=8, color=px.colors.qualitative.Plotly[2]))\n",
    "line_trace = go.Scatter( x=X, y=line, mode='lines', name='regression line', line=dict(color=px.colors.qualitative.Plotly[0]))\n",
    "traces.append(data_trace)\n",
    "traces.append(line_trace)\n",
    "fig.add_traces(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Objective Function: Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the errors, but simply adding them doesn't make sense because a negative 1 error and positive 1 error are equally bad. Simply adding them would cancel each other out, and you'd get 0 error, which is not what we want.\n",
    "\n",
    "Instead we want to square the errors so the negatives become positive before summing them.\n",
    "\n",
    "If you average all the squares of the errors you get mean squared error:\n",
    "$$ \\mathrm{MSE} = \\frac{\\sum_{n=1}^{N} {{e_{n}}^2}}{N} = \\frac{\\sum_{n=1}^{N} { \\left( \\hat{y}_n - y_n \\right) ^2}}{N} = \\frac{\\sum_{n=1}^{N} { \\left( mx_n + b - y_n \\right) ^2}}{N}$$\n",
    "\n",
    "or if you want to do vector/matrix/tensor math:\n",
    "\n",
    "$$ \\mathrm{MSE} = \\frac{ \\bm{e}^\\top \\bm{e}}{N} = \\frac{ \\left( \\hat{\\bm{y}}-\\bm{y} \\right) ^\\top \\left( \\hat{\\bm{y}}-\\bm{y} \\right)}{N} = \\frac{ \\left( \\bm{X}\\bm{w} -\\bm{y} \\right) ^\\top \\left( \\bm{X}\\bm{w} -\\bm{y} \\right)}{N}$$\n",
    "\n",
    "$$ \\bm{X} = \\left[\\begin{array}{cc}\n",
    "    1 & x^{(1)} \\\\\n",
    "    1 & x^{(2)} \\\\\n",
    "    \\vdots \\\\\n",
    "    1 & x^{(N)} \n",
    "   \\end{array}\\right], \\quad\n",
    "    \\bm{w} = \\left[\\begin{array}{cc}\n",
    "    b \\\\\n",
    "    m\n",
    "   \\end{array}\\right] $$\n",
    "\n",
    "This is the function we want to minimize to get the regression line. You want to find the slope ($m$) and intercept ($b$) that minimizes the MSE.\n",
    "\n",
    "In machine learning lingo this is called the **loss** or **objective** function.\n",
    "\n",
    "The variables/parameters you are learning, slope ($b$) and y-intercept ($m$), are called the **weights**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing/optimizing MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MSE in linear regression is a convex function which means it's global minimum value is where its derivative, with respect to slope and y-intercept, is equal to zero.\n",
    "\n",
    "You can solve for where linear regression's MSE derivative equals zero using calculus and algebra (analytically), which is usually the best way to do linear regression.\n",
    "\n",
    "But for many other machine learning algorithms, the loss/objective function cannot be solved analytically. You need to use numerical methods to find the minimum.\n",
    "\n",
    "For educational purposes, we'll do the numerical method called **gradient descent** on MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_surface = torch.linspace(0.0,1.3,100)\n",
    "B_surface = torch.linspace(-1.0,0.5,100)\n",
    "def mse_func(pred_Y, true_Y):\n",
    "    return torch.square(pred_Y - true_Y).mean()\n",
    "MSE_surface = torch.Tensor([[mse_func(m*X + b, Y) for m in M_surface] for b in B_surface])\n",
    "fig = go.Figure()\n",
    "fig.update_layout( height=500, width=750, margin=dict(l=20, r=20, t=20, b=20) )\n",
    "fig.update_scenes(xaxis_title_text='slope',  \n",
    "                yaxis_title_text='intercept',  \n",
    "                zaxis_title_text='MSE',\n",
    "                aspectratio=dict(x=1, y=1, z=0.5),\n",
    "                )\n",
    "surface = go.Surface(x=M_surface, y=B_surface, z=MSE_surface, colorscale='Turbo',\n",
    "                     cmax=1.0,\n",
    "                     cmin=MSE_surface.min().item(), showscale=False, opacity=0.5)\n",
    "fig.add_trace(surface)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initial guess\n",
    "    - Guess $m$ (slope) and $b$ (y-intercept)\n",
    "2. Forward pass\n",
    "    - Predict/calculate $\\hat{y}$ with current $m$ and $b$\n",
    "3. Evaluate loss function\n",
    "    - Keep track of the loss to make sure it's improving.\n",
    "4. Backward pass/backpropagation\n",
    "    - Take the gradient of MSE with respect to $b$ and $m$: $$ \\nabla_{\\bm{w}} MSE = \\left[\\begin{array}{cc}\n",
    "    \\frac{\\partial MSE}{\\partial b} \\\\\n",
    "    \\frac{\\partial MSE}{\\partial m}\n",
    "   \\end{array}\\right] $$\n",
    "\n",
    "        - Pytorch does this for you! Just remember to only do differentiable operations in your forward pass.\n",
    "    - Take a step in the negative direction of the gradient (direction of steepest descent)\n",
    "        - You must pick a learning rate $\\alpha$ which determines how big of a step to take\n",
    "        - $b \\leftarrow b - \\alpha \\frac{\\partial MSE}{\\partial b}$\n",
    "        - $m \\leftarrow m - \\alpha \\frac{\\partial MSE}{\\partial m}$\n",
    "5. Repeat from step 2 until your loss stops decreasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets simply guess 0s for $m$ and $b$\n",
    "- We're going to use pytorch's tensors here\n",
    "    - A vector is a 1D tensor and a matrix is a 2D tensor. A 3D tensor would be matrices stacked on top of each other out the screen. A tensor is the general term for a vector, matrix, or \"matrices\" with higher dimensions.\n",
    "    - Pytorch tensors can keep track of derivatives as you do calculations on it, the requires_grad property needs to be true for this to happen\n",
    "        - Google \"automatic differentiation\" if you want to learn more on how it does this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0.0 # initial m guess\n",
    "b = 0.0 # initial b guess\n",
    "w = torch.tensor([b, m]) # put it in vector\n",
    "w.requires_grad = True # keeps track of gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's compute $\\hat{\\bm{y}} = \\bm{Xw}$\n",
    "- Need to get $\\bm{X}$ in the right form first:\n",
    "$$ \\bm{X} = \\left[\\begin{array}{cc}\n",
    "    1 & x^{(1)} \\\\\n",
    "    1 & x^{(2)} \\\\\n",
    "    \\vdots \\\\\n",
    "    1 & x^{(N)} \n",
    "   \\end{array}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts a column of ones next to a column vector\n",
    "def add_ones_column(input):\n",
    "    ones_vec = torch.ones(input.shape) # make a vector of ones the size of input\n",
    "    output = torch.stack((ones_vec, input), dim=1) # combine the ones vector and input vector, this gives the output matrix\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mat = add_ones_column(X) \n",
    "X_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.matmul(X_mat, w) # make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's graph our predictions with the data to see how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_regression(X, Y, y_pred):\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout( height=500, width=750, margin=dict(l=20, r=20, t=20, b=20), xaxis=dict(title='X'), yaxis=dict(title='Y'))\n",
    "\n",
    "    fig_line = go.Scatter( x=X, y=y_pred, mode='lines', name='regression line', line=dict(color=px.colors.qualitative.Plotly[0]) )\n",
    "    fig_data = go.Scatter( x=X, y=Y, mode='markers', name='data', marker=dict(size=8, color=px.colors.qualitative.Plotly[2]) )\n",
    "\n",
    "    fig.add_traces([fig_data, fig_line])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_regression(X, Y, y_pred.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_all = [] # list to store mse history\n",
    "B = [b] # list to store intercept history\n",
    "M = [m] # list to store slope history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_error = y_pred - Y # calculate residual\n",
    "mse = torch.matmul(torch.t(pred_error), pred_error)/pred_error.numel() # calculate mse\n",
    "\n",
    "print('MSE = ', mse.item())\n",
    "mse_all.append(mse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to choose a learning rate, $\\alpha$. This is how big of a step you take down the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05 # choose a learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.backward()` computes the gradient\n",
    "- the gradients are stored in `.grad`\n",
    "- `w.grad` holds the gradients\n",
    "    - `w.grad[0]` holds $\\partial_b MSE$\n",
    "    - `w.grad[1]` holds $\\partial_m MSE$\n",
    "- update `w`: $$ w \\leftarrow w - \\alpha \\nabla_{\\bm{w}} MSE$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.backward() # compute gradient\n",
    "new_w = w.detach().clone() - w.grad.detach().clone() * learning_rate # take a step\n",
    "print(\"b = \", new_w[0].item())\n",
    "print(\"m = \", new_w[1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep repeating Backward and Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph below will update when you run the block after it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = show_regression(X, Y, y_pred.detach())\n",
    "fig.update_layout(xaxis=dict(range=[-0.5,4.5]), yaxis=dict(range=[-1.5, 4.5]))\n",
    "fig_line_data = fig.data[1]\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph above updates everytime you run the block below. You should see the regression line getting better quickly, then changes slow down as it approaches the regression line that gives the minimal MSE for this data. You can stop re-running the block below when the line is barely changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## forward pass\n",
    "w = new_w\n",
    "w.requires_grad = True\n",
    "y_pred = torch.matmul(X_mat, w) # make a prediction\n",
    "pred_error = y_pred - Y # calculate residual\n",
    "\n",
    "## evaluate loss\n",
    "mse = torch.matmul(torch.t(pred_error), pred_error)/pred_error.numel() # calculate mse\n",
    "# mse.retain_g\n",
    "\n",
    "print('MSE = ', mse.item())\n",
    "mse_all.append(mse.item())\n",
    "B.append(new_w[0].item())\n",
    "M.append(new_w[1].item())\n",
    "\n",
    "## backward pass\n",
    "mse.backward() #\n",
    "new_w = w.detach().clone() - w.grad.detach().clone() * learning_rate\n",
    "print(\"b = \", new_w[0].item())\n",
    "print(\"m = \", new_w[1].item())\n",
    "\n",
    "## update graph\n",
    "if fig_line_data != None:\n",
    "    x_fig = torch.linspace(0,4,10)\n",
    "    x_fig_mat = add_ones_column(x_fig)\n",
    "    y_pred_fig = torch.matmul(x_fig_mat, new_w)\n",
    "    fig_line_data.x = x_fig\n",
    "    fig_line_data.y = y_pred_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot surface of possible values of MSE for different y-intercepts and slopes\n",
    "- Plot the path of our gradient descent on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_surface = torch.linspace(0.0,1.3,100)\n",
    "B_surface = torch.linspace(-1.0,0.5,100)\n",
    "def mse_func(pred_Y, true_Y):\n",
    "    return torch.square(pred_Y - true_Y).mean()\n",
    "MSE_surface = torch.Tensor([[mse_func(m*X + b, Y) for m in M_surface] for b in B_surface])\n",
    "fig = go.Figure()\n",
    "fig.update_layout( height=500, width=750, margin=dict(l=20, r=20, t=20, b=20) )\n",
    "fig.update_scenes(xaxis_title_text='slope',  \n",
    "                yaxis_title_text='intercept',  \n",
    "                zaxis_title_text='MSE',\n",
    "                # yaxis = dict(range=[-1,1.5]),\n",
    "                # zaxis = dict(range=[MSE.min().item(),MSE_max_display]),\n",
    "                aspectratio=dict(x=1, y=1, z=0.5),\n",
    "                )\n",
    "surface = go.Surface(x=M_surface, y=B_surface, z=MSE_surface, colorscale='Turbo',\n",
    "                     cmax=1.0,\n",
    "                     cmin=MSE_surface.min().item(), showscale=False, opacity=0.5)\n",
    "fig.add_trace(surface)\n",
    "gradient_descent = go.Scatter3d(x=M, y=B, z=mse_all, line=dict(color='white'), marker=dict(size=2))\n",
    "fig.add_trace(gradient_descent)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The plot above is for educational purposes \n",
    "- In most situations you will have more than two unknowns (or weights, paramaters, whatever you want to call it) that you are trying to learn, then you can't make the above plot\n",
    "- Instead plot the loss vs iterations\n",
    "- You should see loss decreasing and hopefully converging to a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line( x=range(len(mse_all)), y=mse_all, title='MSE v gradient descent steps' )\n",
    "fig.update_layout( autosize=False, width=750, height=500 )\n",
    "fig.update_xaxes( title=\"steps\" )\n",
    "fig.update_yaxes( title='MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: follow the same steps on the dataset below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.load('X_data.pt')\n",
    "Y=torch.load('Y_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.update_layout( height=500, width=750, margin=dict(l=20, r=20, t=20, b=20), xaxis=dict(title='X'), yaxis=dict(title='Y'))\n",
    "fig_data = go.Scatter( x=X, y=Y, mode='markers', name='data' )\n",
    "\n",
    "fig.add_traces([fig_data])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
