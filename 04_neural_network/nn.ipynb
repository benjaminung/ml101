{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get's name from neurons in biology\n",
    "- One neuron receives signals from other neurons, combines them in some way, then sends this combined signal to other neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can make neural nets any size you want\n",
    "    - add/subtract **hidden layers** (making the diagram above wider/thinnier). This is what's known as **depth** of the neural net.\n",
    "    - add/subtract **nodes** in a hidden layer (making the the diagram taller/shorter)\n",
    "- each hidden layer performs a linear operation, then a non-linear operation called an **activation function**\n",
    "- the sigmoid function is one of many different types of activation functions you can use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math operations in forward pass of neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer and hidden layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\bm{Z} = \\bm{\\sigma}(\\bm{X} \\bm{W} + \\bm{J}_{N,2} \\bm{w}_b) $$\n",
    "- where:\n",
    "    \n",
    "    - $\\bm{\\sigma}(\\bm{A})$ is the sigmoid function applied element-wise to matrix $\\bm{A}$:\n",
    "        $$\\bm{\\sigma}(\\bm{A}) = \\left[\\begin{array}{cc}\n",
    "                \\sigma(a_1^{(1)}) & \\sigma(a_2^{(1)}) & \\cdots & \\sigma(a_{I}^{(1)}) \\\\\n",
    "                \\sigma(a_1^{(2)}) & \\sigma(a_2^{(2)}) & \\cdots & \\sigma(a_{I}^{(2)}) \\\\\n",
    "                \\vdots            & \\vdots            & \\ddots & \\vdots\\\\\n",
    "                \\sigma(a_1^{(N)}) & \\sigma(a_2^{(N)}) & \\cdots & \\sigma(a_{I}^{(N)})\n",
    "                \\end{array}\\right] $$\n",
    "    - reminder that the sigmoid function is:\n",
    "        $$ \\sigma(a) = \\frac{1}{1 + e^{-a}} $$\n",
    "    - $N$ is the number of data points\n",
    "    - $\\bm{J}_{a,b}$ is a matrix with size $a \\times b$ full of $1$'s\n",
    "        - e.g. :\n",
    "        $$\\bm{J}_{4,2} = \\left[\\begin{array}{cc}\n",
    "            1 & 1\\\\\n",
    "            1 & 1\\\\\n",
    "            1 & 1\\\\\n",
    "            1 & 1\n",
    "            \\end{array}\\right]$$\n",
    "    - $\\bm{w}_b$ is the bias term:\n",
    "        $$ \\bm{w}_b = \\left[\\begin{array}{cc}\n",
    "            w_{01} & w_{02}\n",
    "            \\end{array}\\right] $$\n",
    "    - $\\bm{X}$ and $\\bm{W}$: \n",
    "        $$ \\bm{X} = \\left[\\begin{array}{cc}\n",
    "            x_1^{(1)} & x_2^{(1)} & x_{3}^{(1)} \\\\\n",
    "            x_1^{(2)} & x_2^{(2)} & x_{3}^{(2)} \\\\\n",
    "            \\vdots    & \\vdots    & \\vdots\\\\\n",
    "            x_1^{(N)} & x_2^{(N)} & x_{3}^{(N)}\n",
    "            \\end{array}\\right], \\quad \\quad \n",
    "            \\bm{W} = \\left[\\begin{array}{cc}\n",
    "            w_{11} & w_{12}\\\\\n",
    "            w_{21} & w_{22}\\\\\n",
    "            w_{31} & w_{32}\n",
    "            \\end{array}\\right] $$\n",
    "- $\\bm{Z}$ is a matrix of size $N \\times 2$:\n",
    "    $$ \\bm{Z} \\in \\mathbb{R}^{N \\times 2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the code below makes hidden layer 1 with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "layer1 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=3, out_features=2),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the code below gets the bias terms for hidden layer 1 (i.e. $w_{01}$ and $w_{02}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bias = layer1[0].bias.data\n",
    "print(w_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the code below gets $\\bm{W}$\n",
    "- notice that pytorch stores it as the transpose of $\\bm{W}$ above\n",
    "  - $ \\bm{W}^\\top = \\left[\\begin{array}{cc}\n",
    "            w_{11} & w_{21} & w_{31}\\\\\n",
    "            w_{12} & w_{22} & w_{32}\n",
    "            \\end{array}\\right] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_weight = layer1[0].weight.data\n",
    "print(w_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\bm{S} = \\bm{\\sigma}(\\bm{Z}\\bm{V} + \\bm{J}_{N,3} \\bm{v}_b) $$\n",
    "$$ \\bm{S} \\in \\mathbb{R}^{N \\times 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the pytorch code for hidden layer 2 is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=2, out_features=3),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{\\bm{Y}} = \\bm{\\sigma}(\\bm{S}\\bm{T} + \\bm{J}_{N,2} \\bm{t}_b) $$\n",
    "$$ \\hat{\\bm{Y}} \\in \\mathbb{R}^{N \\times 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the pytorch code for the output layer is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=3, out_features=2),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together in one python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleNN(torch.nn.Module): # inherits from torch.nn.Module\n",
    "    def __init__(self): # defines initialization of this module\n",
    "        super().__init__() # calls the parent class' (torch.nn.Module) initializer\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=3, out_features=2), # linear operation\n",
    "            torch.nn.Sigmoid() # activation\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=2, out_features=3),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.layer_output = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=3, out_features=2),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): # forward pass through neural network\n",
    "        z = self.layer1(x)\n",
    "        s = self.layer2(z)\n",
    "        y = self.layer_output(s)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- instantiate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_nn = ExampleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make a fake dataset to pass through the neural net\n",
    "    - dataset has 20 examples, with 3 features for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones(20,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- do a forward pass with the fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = example_nn(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the output should be a $20 \\times 2$ matrix:\n",
    "  \n",
    "  $\\hat{\\bm{Y}} \\in \\mathbb{R}^{20 \\times 2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^{2x} -1}{e^{2x} + 1}$$\n",
    "- similar shape to sigmoid\n",
    "- often times converges to an optimum value faster than sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(-10.0, 10, 100)\n",
    "tanh = torch.nn.Tanh()\n",
    "Y = tanh(X)\n",
    "\n",
    "## we could also use the torch.nn.functional version so we don't have to make an object of torch.nn.Tanh class\n",
    "Y = torch.nn.functional.tanh(X)\n",
    "\n",
    "px.line(x=X, y=Y, title=\"Tanh\", labels={'x':'input', 'y':'output'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified linear unit (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{relu}(x) =  \\begin{cases} \n",
    "      x, & \\text{if} \\ \\ x \\geq 0 \\\\\n",
    "      0, & \\text{if} \\ \\ x <    0\n",
    "   \\end{cases} $$\n",
    "- quick to compute\n",
    "- no vanishing gradient when input is large\n",
    "- has vanishing gradient when input is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(-10.0, 10, 100)\n",
    "Y = torch.nn.functional.relu(X)\n",
    "px.line(x=X, y=Y, title=\"ReLU\", labels={'x':'input', 'y':'output'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{LeakyReLU}(x) =  \\begin{cases} \n",
    "      x, & \\text{if} \\ \\ x \\geq 0 \\\\\n",
    "      mx, & \\text{if} \\ \\ x <    0\n",
    "   \\end{cases} $$\n",
    "$$ \\text{where} \\quad 0 < m < 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- helps avoid vanishing gradients in ReLU by giving negative inputs a small slope\n",
    "- still pretty simple to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(-10.0, 10, 100)\n",
    "Y = torch.nn.functional.leaky_relu(X, negative_slope=0.1)\n",
    "px.line(x=X, y=Y, title=\"Leaky ReLU\", labels={'x':'input', 'y':'output'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{PReLU}(x) =  \\begin{cases} \n",
    "      x, & \\text{if} \\ \\ x \\geq 0 \\\\\n",
    "      mx, & \\text{if} \\ \\ x <    0\n",
    "   \\end{cases} $$\n",
    "$$ m \\text{ is a learnable parameter} $$\n",
    "- same as Leaky ReLU but the $m$ is a learnable parameter, like the weights in the linear operations of neural net layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you're predicting a continuous output \n",
    "  - default loss function is mean square error\n",
    "- If you're predciting classes (one or more)\n",
    "  - use binary cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Neural Net with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to train a neural net to predict the Y in the data below. This data has one dimension in the input and one dimension in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = torch.load('nn_dataframe.pt')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x='x', y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a neural net that has two hidden layers, with three nodes in the first hidden layer and two nodes in the second. Use PReLU activation functions.\n",
    "\n",
    "Normally, you'd have to determine how many layers, how many nodes, and which activation function to use by **hyperparameter tuning**, but for the first neural net example, to make it simple, I'll give you the right hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module): # inherits from torch.nn.Module\n",
    "    def __init__(self): # defines initialization of this module\n",
    "        super().__init__() # calls the parent class' (torch.nn.Module) initializer\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=1, out_features=3),\n",
    "            torch.nn.PReLU(),\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=3, out_features=2),\n",
    "            torch.nn.PReLU(),\n",
    "        )\n",
    "        self.layer_output = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=2, out_features=1),\n",
    "            torch.nn.PReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): # forward pass through neural network\n",
    "        z = self.layer1(x)\n",
    "        s = self.layer2(z)\n",
    "        y = self.layer_output(s)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    assert isinstance(data, torch.Tensor) # check that data is a pytorch tensor\n",
    "    column_mu = data.mean(dim=0) # take the mean of each column\n",
    "    column_sigma = data.std(dim=0) # take the std deviation of each column\n",
    "    data_standardized = (data-column_mu)/column_sigma # subract each values column mean then divide by the column std deviation\n",
    "    return data_standardized, column_mu, column_sigma\n",
    "\n",
    "def unstandardize(data_stand, column_mu, column_sigma):\n",
    "    assert isinstance(data_stand, torch.Tensor)\n",
    "    return data_stand * column_sigma + column_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, x_mu, x_sigma = standardize(torch.tensor(df['x'].values))\n",
    "X_all = X_all\n",
    "Y_all, y_mu, y_sigma = standardize(torch.tensor(df['y'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_order_indices = torch.randperm(len(X_all)) # random order of X_all's indices\n",
    "train_proportion = 0.7\n",
    "valid_proportion = 0.15\n",
    "test_proportion = 0.15\n",
    "train_valid_index_border = torch.math.floor(len(X_all)*train_proportion)\n",
    "valid_test_index_border = torch.math.floor(len(X_all)*(train_proportion+valid_proportion))\n",
    "train_indices = random_order_indices[0:train_valid_index_border] # take first 80% of random_order_indices to be the train indices\n",
    "valid_indices = random_order_indices[train_valid_index_border:valid_test_index_border]\n",
    "test_indices = random_order_indices[valid_test_index_border:]\n",
    "\n",
    "X = X_all[train_indices]\n",
    "Y = Y_all[train_indices]\n",
    "X_valid = X_all[valid_indices]\n",
    "Y_valid = Y_all[valid_indices]\n",
    "X_test  = X_all[test_indices]\n",
    "Y_test  = Y_all[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doesn't use all the dataset at once\n",
    "- Breaks data into batches\n",
    "- Trains model on one batch at a time\n",
    "- Useful when your dataset is very large and/or when your neural net requires a lot of memory to do forward or backward passes\n",
    "- Also helps generalize model better\n",
    "- After going through all data in the dataset once, shuffle the data and repeat\n",
    "- Each round through the entire dataset is called an **epoch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pytorch's Dataset and DataLoader for Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataLoader does the batching and shuffling for you\n",
    "- Data needs to be in a pytorch Dataset class to use DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetData(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].float(), self.Y[idx].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A dataset class needs the three functions you see above\n",
    "- If you are using SGD to save on memory usage, do not load the entire dataset when creating the dataset class like above. You'll need grab the data from it's location on your computer storage in the `__getitem__` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the batching to work in pytorch all datasets need to have at least two dimensions.\n",
    "\n",
    "First dimension is the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.unsqueeze(1).float()\n",
    "Y = Y.unsqueeze(1).float()\n",
    "X_valid = X_valid.unsqueeze(1).float()\n",
    "Y_valid = Y_valid.unsqueeze(1).float()\n",
    "X_test = X_test.unsqueeze(1).float()\n",
    "Y_test = Y_test.unsqueeze(1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NeuralNetData(X, Y)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- change the step function from previous lessons to include a loop to iterate over batches\n",
    "- we'll only evaluate with validation data after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(train_dataloader, X_valid, Y_valid, model, optim, Losses, Losses_valid):\n",
    "    global epoch\n",
    "    print('epoch #{}'.format(epoch))\n",
    "    for batch in train_dataloader:\n",
    "        optim.zero_grad() # reset the gradients\n",
    "        model.train() # tells pytorch we're training the linear module\n",
    "        X = batch[0]\n",
    "        Y = batch[1]\n",
    "        pred_y = model(X) # make a prediction\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(pred_y, Y) # calculate mse loss\n",
    "        loss.backward() # calculate gradients\n",
    "        optim.step() # take gradient descent step\n",
    "        Losses.append(loss.item()) # add current training loss to Losses list\n",
    "\n",
    "    model.eval() # tells pytorch we're not training our module (we're validating now instead of training)\n",
    "    with torch.no_grad(): # Tells pytorch to not track gradients. This prevents unnecessary calculations and memory usage\n",
    "        pred_y_valid = model(X_valid)\n",
    "        loss_valid = torch.nn.functional.mse_loss(pred_y_valid, Y_valid)\n",
    "    \n",
    "    Losses_valid.append(loss_valid.item()) # add current validation loss to Losses_valid list\n",
    "    print('Validation Loss: ' + str(loss_valid.item()))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_steps(title):\n",
    "    Losses = []\n",
    "    Losses_valid = []\n",
    "    fig = go.FigureWidget()\n",
    "    \n",
    "    trace_train = go.Scatter( x=list(range(len(Losses))), y=Losses, line=dict(color=px.colors.qualitative.Plotly[0]), name='train loss' )\n",
    "    fig.add_trace(trace_train)\n",
    "    \n",
    "    trace_valid = go.Scatter( x=list(range(len(Losses_valid))), y=Losses_valid, line=dict(color=px.colors.qualitative.Plotly[1]), name='valid loss' )\n",
    "    fig.add_trace(trace_valid)\n",
    "    \n",
    "    fig.update_layout( autosize=False, width=750, height=500, title=title)\n",
    "    fig.update_xaxes( title=\"steps\")\n",
    "    fig.update_yaxes( title='loss')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = NeuralNet() # initialize neural net\n",
    "optim = torch.optim.SGD(neural_net.parameters(), lr=0.05)\n",
    "epoch = 0\n",
    "step = 0\n",
    "Losses = []\n",
    "Losses_valid = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural nets usually have many local minimums, the larger your neural net the more minimums you'll have\n",
    "- The values you initialize the weights with have a huge affect on the results you get\n",
    "- We're going to use Kaiming/He initialization which works well with ReLU activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "neural_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = graph_steps('MSE Loss')\n",
    "train_loss = fig.data[0] # use this variable to add train losses to graph\n",
    "valid_loss = fig.data[1] # use this variable to add validation losses to graph\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_epoch(train_dataloader, X_valid, Y_valid, neural_net, optim, Losses, Losses_valid)\n",
    "# if Losses_valid[-1] < best_model_valid_loss:\n",
    "#     best_model = copy_linear(model)\n",
    "#     best_model = Losses_valid[-1]\n",
    "train_loss.x = list(range(len(Losses))) # add the train steps to graph on x-axis\n",
    "train_loss.y = Losses # add the train losses to graph on y-axis\n",
    "valid_loss.x = list(range(len(train_dataloader)-1,(len(Losses_valid)+1)*len(train_dataloader), len(train_dataloader)))\n",
    "valid_loss.y = Losses_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate MSE on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_y = neural_net(X_test)\n",
    "test_mse = torch.nn.functional.mse_loss(test_pred_y, Y_test)\n",
    "test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the regression line your neural net produced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will import the actual underlying trend line, plot the data generated from this trend line, and plot the line your neural net predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trend = torch.load('nn_dataframe_trend.pt')\n",
    "trend_X = df_trend['x'].values\n",
    "trend_Y = df_trend['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_X = torch.linspace(X_all.min(), X_all.max(), 1000).unsqueeze(1)\n",
    "line_Y = neural_net(line_X)\n",
    "line_X = unstandardize(line_X, x_mu, x_sigma)\n",
    "line_Y = unstandardize(line_Y, y_mu, y_sigma)\n",
    "X_all_unstd = unstandardize(X_all, x_mu, x_sigma)\n",
    "Y_all_unstd = unstandardize(Y_all, y_mu, y_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "data_trace = go.Scatter(x=X_all_unstd.detach().squeeze(), y=Y_all_unstd.detach().squeeze(), mode='markers', name='data')\n",
    "line_trace = go.Scatter(x=line_X.squeeze().detach(), y=line_Y.squeeze().detach(), mode='lines', name='nn prediction')\n",
    "trend_trace = go.Scatter(x=trend_X, y=trend_Y, mode='lines', name='actual trend')\n",
    "fig.add_traces([data_trace, line_trace, trend_trace])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a neural net of logistic regression with 4 input features\n",
    "\n",
    "*Answer is in file `log_reg_4x_nn_diagram.png`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each element of `training_data` and `test_data` is a tuple with the image and its label\n",
    "- The first element of the tuple is the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_0 = training_data[0][0]\n",
    "image_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch image format is the first dimension is channels, second is height, third is width\n",
    "- `px.imshow` needs a two dimension array, so squeeze that thrid dimension away to display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(image_0.squeeze(), color_continuous_scale='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second element of the tuple is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand the dataset, make a neural net to predict the digit from the images.\n",
    "\n",
    "You may want to research convolutional layers to do this task. Convolutional layers can take advantage of the grid structure of images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
