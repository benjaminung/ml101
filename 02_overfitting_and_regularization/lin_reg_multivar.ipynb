{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting is when your model is too complicated and can't generalize to unseen data well\n",
    "- If you are overfitting in linear regression, this could meanyou have too many features/inputs in your model (too many x variables)\n",
    "- You're fitting to noise rather than the underlying data trend\n",
    "- You can think of overfitting as memorizing data rather than learning the trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with many inputs/independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data below has 100 people's salaries (in thousands of dollars), years of work experience, years of education, number of pizza slices eaten in a year, and 100 other random info about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = torch.load('multivar_lin_reg.pt')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The true output or $\\bm{y}$ is the `salaries` column\n",
    "\n",
    "- Let's assume you don't know which of the other columns are correlated with `salaries`, and you decide to run a linear regression with all 103 columns as the $x$ variables.\n",
    "\n",
    "- You'll use mean square error as your objective/loss function\n",
    "\n",
    "- You'll try to learn the $b$ and $m$'s (intercept/bias and slopes/coefficents) that minimizes/optimizes our loss/objective function:\n",
    "  $$\\underset{b, m_1, m_2, \\dots, m_{103}}{\\operatorname{argmin}} \\  \\frac{1}{N} \\sum_{n=1}^{N} { \\left( \\hat{y}^{(n)} - y^{(n)} \\right) ^2} $$\n",
    "  - where:\n",
    "  $$ \\hat{y}^{(n)} = b + m_1 x_1^{(n)} + m_2 x_2^{(n)} + ... + m_{103} x_{103}^{(n)} $$\n",
    "\n",
    "- or in matrix form:\n",
    "\n",
    "  $$ \\underset{\\bm{w}}{\\operatorname{argmin}} \\  \\frac{1}{N} (\\hat{\\bm{y}}-\\bm{y})^\\top (\\hat{\\bm{y}}-\\bm{y})  $$\n",
    "\n",
    "    - where\n",
    "    \n",
    "    $$ \\hat{\\bm{y}} = \\bm{X}\\bm{w} $$\n",
    "\n",
    "    $$ \\bm{X} = \\left[\\begin{array}{cc}\n",
    "    1 & x_1^{(1)} & x_2^{(1)} & \\dots & x_{103}^{(1)} \\\\\n",
    "    1 & x_1^{(2)} & x_2^{(2)} & \\dots & x_{103}^{(2)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots  & \\vdots\\\\\n",
    "    1 & x_1^{(100)} & x_2^{(100)} & \\dots & x_{103}^{(100)}\n",
    "   \\end{array}\\right], \\quad \\quad \n",
    "    \\bm{w} = \\left[\\begin{array}{cc}\n",
    "    b \\\\\n",
    "    m_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    m_{103}\n",
    "   \\end{array}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- take the mean, $\\mu$ and standard deviation, $\\sigma$, of each column\n",
    "- subtract every value by its column's mean\n",
    "- then divide each value by its column's standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why standardize data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to get every variable on the same scale\n",
    "  - our `salaries` column goes from about 0-400, `exp_years` 0-40, `ed_years` 9-22, \n",
    "  - every column will have $\\mu=0$ and $\\sigma=1$ after standardization\n",
    "- makes gradient descent behave numerically better\n",
    "  - less likely to lose precision from very large or very small gradients\n",
    "- necessary with **regularization** (will be discussed later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    assert isinstance(data, torch.Tensor) # check that data is a pytorch tensor\n",
    "    column_mu = data.mean(dim=0) # take the mean of each column\n",
    "    column_sigma = data.std(dim=0) # take the std deviation of each column\n",
    "    data_standardized = (data-column_mu)/column_sigma # subract each values column mean then divide by the column std deviation\n",
    "    return data_standardized, column_mu, column_sigma\n",
    "\n",
    "def unstandardize(data_stand, column_mu, column_sigma):\n",
    "    assert isinstance(data_stand, torch.Tensor)\n",
    "    return data_stand * column_sigma + column_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stand, mu, sig = standardize(torch.tensor(df.values))\n",
    "df_stand = pd.DataFrame(data_stand).astype(float)\n",
    "df_stand.columns = df.columns\n",
    "df_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The train data is what we use to do the optimization/minimization of the objective/loss function\n",
    "    - Last lesson everything was training data\n",
    "- The validation data is what we use to check if we're overfitting during training\n",
    "- The test data is what we use to report how well our model does\n",
    "    - You should only use test data once\n",
    "    - If you don't like the results on the test data:\n",
    "        - turn the test data into training or validation data and re-train the model\n",
    "        - get new data that you've never seen before to be the new test data\n",
    "- If you are overfitting, you will see your loss function increasing with the validation data while decreasing with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = torch.tensor(df_stand.iloc[:,1:].values)\n",
    "Y_all = torch.tensor(df_stand['salaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred_Y, true_Y):\n",
    "    assert pred_Y.shape == true_Y.shape\n",
    "    return torch.square(pred_Y - true_Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_order_indices = torch.randperm(len(X_all)) # random order of X_all's indices\n",
    "train_proportion = 0.7\n",
    "valid_proportion = 0.15\n",
    "test_proportion = 0.15\n",
    "train_valid_index_border = torch.math.floor(len(X_all)*train_proportion)\n",
    "valid_test_index_border = torch.math.floor(len(X_all)*(train_proportion+valid_proportion))\n",
    "train_indices = random_order_indices[0:train_valid_index_border] # take first 80% of random_order_indices to be the train indices\n",
    "valid_indices = random_order_indices[train_valid_index_border:valid_test_index_border]\n",
    "test_indices = random_order_indices[valid_test_index_border:]\n",
    "\n",
    "X = X_all[train_indices]\n",
    "Y = Y_all[train_indices]\n",
    "X_valid = X_all[valid_indices]\n",
    "Y_valid = Y_all[valid_indices]\n",
    "X_test  = X_all[test_indices]\n",
    "Y_test  = Y_all[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure distribution of each set is similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.update_layout( height=500, width=750, margin=dict(l=20, r=20, t=20, b=20) )\n",
    "train_trace = go.Histogram( x=Y, histnorm='percent', name='train' )\n",
    "valid_trace = go.Histogram( x=Y_valid, histnorm='percent', name='validation')\n",
    "test_trace = go.Histogram( x=Y_test, histnorm='percent', name='test')\n",
    "fig.add_traces([train_trace, valid_trace, test_trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit to the random data columns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pytorch's linear module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This time we'll use pytorch's Linear module and pytorch's gradient descent optimizer\n",
    "\n",
    "- The Linear module stores the $b$ and $m$'s separately in `bias` and `weight`properties of the module\n",
    "\n",
    "- make a prediciton by calling the module with input data as the paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = torch.nn.Linear(in_features=103, out_features=1) # in_features is the number of x-variables, out_features is the number of outputs per data entry\n",
    "line.weight # contains the coefficients of the inputs/x variables\n",
    "line.bias # contains the bias/y-intercept\n",
    "line(X[0,:].float()) # make a predicion with the current weight and bias using the first train data entry as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pytorch's gradient descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we're going to use pytorch's Stochastic Gradient Descent method to do gradient descent\n",
    "\n",
    "    - *Stochastic* part of this will be explained in a later lesson\n",
    "    \n",
    "    - we're using this as just a regular gradient descent here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(line.parameters(), lr=0.05) # use gradient descent with learning rate of 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make a function to do a step of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, Y, X_valid, Y_valid, line, optim, Losses, Losses_valid):\n",
    "    global epoch\n",
    "    print('epoch #{}'.format(epoch))\n",
    "    optim.zero_grad() # reset the gradients\n",
    "    line.train() # tells pytorch we're training the linear module\n",
    "\n",
    "    pred_y = line(X.float()).squeeze() # make a prediction\n",
    "    # squeeze removes dimensions with size 1\n",
    "    #   e.g. squeeze applied to a tensor with shape (3,1) will return a tensor with shape (3)\n",
    "\n",
    "    loss = mse(pred_y, Y) # calculate mse\n",
    "    loss.backward() # calculate gradients\n",
    "    optim.step() # take gradient descent step\n",
    "\n",
    "    line.eval() # tells pytorch we're not training our linear module (we're validating now instead of training)\n",
    "    with torch.no_grad(): # Tells pytorch to not track gradients. This prevents unnecessary calculations and memory usage\n",
    "        pred_y_valid = line(X_valid.float()).squeeze()\n",
    "        loss_valid = mse(pred_y_valid, Y_valid)\n",
    "\n",
    "    Losses.append(loss.item()) # add current training loss to Losses list\n",
    "    Losses_valid.append(loss_valid.item()) # add current validation loss to Losses_valid list\n",
    "    print('Validation Loss: ' + str(loss_valid.item()))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make a function to plot the MSE v gradient descent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_steps(title):\n",
    "    Losses = []\n",
    "    Losses_valid = []\n",
    "    fig = go.FigureWidget()\n",
    "    trace_train = go.Scatter( x=list(range(len(Losses))), y=Losses, line=dict(color=px.colors.qualitative.Plotly[0]), name='train' )\n",
    "    fig.add_trace(trace_train)\n",
    "    trace_valid = go.Scatter( x=list(range(len(Losses_valid))), y=Losses_valid, line=dict(color=px.colors.qualitative.Plotly[1]), name='valid' )\n",
    "    fig.add_trace(trace_valid)\n",
    "    # color=px.colors.qualitative.Plotly[1] \n",
    "    # fig.add_traces( list( px.line( x=range(len(Losses_valid)), y=Losses_valid, color=px.colors.qualitative.Plotly[1]  ).select_traces() ) )\n",
    "    fig.update_layout( autosize=False, width=750, height=500, title=title )\n",
    "    fig.update_xaxes( title=\"steps\" )\n",
    "    fig.update_yaxes( title='MSE')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create function to copy linear models. Use this to store your best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_linear(linear):\n",
    "    in_features = linear.weight.shape[1]\n",
    "    out_features = linear.weight.shape[0]\n",
    "    linear_copy = torch.nn.Linear(in_features, out_features)\n",
    "    linear_copy.load_state_dict(linear.state_dict())\n",
    "    return linear_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model until it overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses = []\n",
    "Losses_valid = []\n",
    "epoch = 0\n",
    "best_line = None # best_line holds the best model learned\n",
    "best_line_valid_loss = torch.inf # holds the loss value on validation data from the best_line\n",
    "fig = graph_steps('Linear regression gradient descent')\n",
    "train_loss = fig.data[0] # use this variable to add train losses to graph\n",
    "valid_loss = fig.data[1] # use this variable to add validation losses to graph\n",
    "fig # display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step(X, Y, X_valid, Y_valid, line, optim, Losses, Losses_valid) # take a gradient descent step\n",
    "if Losses_valid[-1] < best_line_valid_loss:\n",
    "    best_line = copy_linear(line)\n",
    "    best_line_valid_loss = Losses_valid[-1]\n",
    "train_loss.x = list(range(len(Losses))) # add the train steps to graph on x-axis\n",
    "train_loss.y = Losses # add the train losses to graph on y-axis\n",
    "valid_loss.x = list(range(len(Losses_valid)))\n",
    "valid_loss.y = Losses_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep running the code block above until the MSE for the validation data is increasing while the training data MSE is decreasing.\n",
    "\n",
    "This means you are overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization are methods used to prevent overfitting\n",
    "- There are two main regularization techniques for linear regression: L1/Lasso and L2/Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- steps:\n",
    "\n",
    "    - Take the absolute value of all the weights/slopes\n",
    "\n",
    "    \n",
    "    - Scale it by mulitplying by a regularization factor, $\\lambda$, that you choose\n",
    "    \n",
    "    - Add it to your MSE\n",
    "\n",
    "- Your regularized loss/objective function will then be:\n",
    "    \n",
    "    $$\\underset{b, m_1, m_2, \\dots, m_{I}}{\\operatorname{argmin}} \\  \\frac{1}{N} \\sum_{n=1}^{N} { \\left( \\hat{y}^{(n)} - y^{(n)} \\right) ^2} + \\lambda \\sum_{i=1}^{I}{ \\left| m_i \\right| } $$\n",
    "    \n",
    "    - where:\n",
    "    \n",
    "        - $N$ is the number of data samples and\n",
    "    \n",
    "        - $I$ is the number of independent/x-variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar to L1 Lasso regularization, but square the values instead of taking the absolute value\n",
    "\n",
    "- Your regularized loss/objective function with L2/Ridge regularization will be:\n",
    "\n",
    "    $$\\underset{b, m_1, m_2, \\dots, m_{I}}{\\operatorname{argmin}} \\  \\frac{1}{N} \\sum_{n=1}^{N} { \\left( \\hat{y}^{(n)} - y^{(n)} \\right) ^2} + \\lambda \\sum_{i=1}^{I}{m_i^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use L1 vs L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\lambda$ controls the strength of regularization for both:\n",
    "- $\\lambda=0$ is the same as no regularization\n",
    "- L1/Lasso regression drives coefficients/weights/slopes to zero\n",
    "  - The higher the $\\lambda$ the more coefficients will become zero\n",
    "- L2/Rigde regression is good to use when your x-variables are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding L1 and L2 loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred_Y, true_Y):\n",
    "    assert pred_Y.shape == true_Y.shape\n",
    "    return torch.square(pred_Y - true_Y).mean()\n",
    "\n",
    "def mse_l1(pred_Y, true_Y, linear, lam):\n",
    "    assert pred_Y.shape == true_Y.shape\n",
    "    reg = lam * linear.weight.abs().sum()\n",
    "    return torch.square(pred_Y - true_Y).mean() + reg\n",
    "\n",
    "def mse_l2(pred_Y, true_Y, linear, lam):\n",
    "    assert pred_Y.shape == true_Y.shape\n",
    "    reg = lam * linear.weight.square().sum()\n",
    "    return torch.square(pred_Y - true_Y).mean() + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- change the step function to change the loss function based on the regularizer you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, Y, X_valid, Y_valid, lam, line, optim, regularizer, Losses, Losses_valid):\n",
    "    global epoch\n",
    "    print('epoch #{}'.format(epoch))\n",
    "    optim.zero_grad() # reset the gradients\n",
    "    line.train()\n",
    "\n",
    "    pred_y = line(X.float()).squeeze() #\n",
    "\n",
    "    if regularizer == None:\n",
    "        loss = mse(pred_y, Y)\n",
    "    elif regularizer == 'l1':\n",
    "        loss = mse_l1(pred_y, Y, line, lam)\n",
    "    elif regularizer == 'l2':\n",
    "        loss = mse_l2(pred_y, Y, line, lam)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    line.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_y_valid = line(X_valid.float()).squeeze()\n",
    "        loss_valid = mse(pred_y_valid, Y_valid)\n",
    "    line.train()\n",
    "    \n",
    "    Losses.append(loss.item())\n",
    "    Losses_valid.append(loss_valid.item())\n",
    "    print('Validation Loss: ' + str(loss_valid.item()))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train L1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses_l1 = []\n",
    "Losses_valid_l1 = []\n",
    "epoch = 0\n",
    "lam = 0.25\n",
    "line_l1 = torch.nn.Linear(in_features=X.shape[1], out_features=1)\n",
    "best_line_l1 = None # best_line holds the best model learned\n",
    "best_line_valid_loss_l1 = torch.inf # holds the loss value on validation data from the best_line\n",
    "optim_l1 = torch.optim.SGD(line_l1.parameters(), lr=0.01) # use gradient descent  with learning rate of 0.1\n",
    "regularizer = 'l1'\n",
    "fig_l1 = graph_steps(regularizer)\n",
    "train_loss_l1 = fig_l1.data[0]\n",
    "valid_loss_l1 = fig_l1.data[1]\n",
    "fig_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step(X, Y, X_valid, Y_valid, lam, line_l1, optim_l1, regularizer, Losses_l1, Losses_valid_l1)\n",
    "if Losses_valid_l1[-1] < best_line_valid_loss_l1:\n",
    "    best_line_l1 = copy_linear(line_l1)\n",
    "    best_line_valid_loss_l1 = Losses_valid_l1[-1]\n",
    "train_loss_l1.x = list(range(len(Losses_l1)))\n",
    "train_loss_l1.y = Losses_l1\n",
    "valid_loss_l1.x = list(range(len(Losses_valid_l1)))\n",
    "valid_loss_l1.y = Losses_valid_l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the L2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses_l2 = []\n",
    "Losses_valid_l2 = []\n",
    "epoch = 0\n",
    "lam = 0.25\n",
    "line_l2 = torch.nn.Linear(in_features=X.shape[1], out_features=1)\n",
    "best_line_l2 = None # best_line holds the best model learned\n",
    "best_line_valid_loss_l2 = torch.inf # holds the loss value on validation data from the best_line\n",
    "optim_l2 = torch.optim.SGD(line_l2.parameters(), lr=0.05) # use gradient descent  with learning rate of 0.1\n",
    "regularizer = 'l2'\n",
    "fig_l2 = graph_steps(regularizer)\n",
    "train_loss_l2 = fig_l2.data[0]\n",
    "valid_loss_l2 = fig_l2.data[1]\n",
    "fig_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step(X, Y, X_valid, Y_valid, lam, line_l2, optim_l2, regularizer, Losses_l2, Losses_valid_l2)\n",
    "if Losses_valid_l2[-1] < best_line_valid_loss_l2:\n",
    "    best_line_l2 = copy_linear(line_l2)\n",
    "    best_line_valid_loss_l2 = Losses_valid_l2[-1]\n",
    "train_loss_l2.x = list(range(len(Losses_l2)))\n",
    "train_loss_l2.y = Losses_l2\n",
    "valid_loss_l2.x = list(range(len(Losses_valid_l2)))\n",
    "valid_loss_l2.y = Losses_valid_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to ground-truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The only two columns/variables correlated with salaries is education and experience\n",
    "\n",
    "- I only know this because I created the data. It's not real world data.\n",
    "\n",
    "- In a real dataset you will not know the actual true trend. (If you already knew it, you wouldn't need to do a linear regression!)\n",
    "\n",
    "- The underlying line (unstandardized) I used to create this data is:\n",
    "  $$y = -61.3846 + 9.8462 x_1 + 7.6154 x_2$$\n",
    "  - $y$ is `salaries`\n",
    "  - $x_1$ is `exp_years`\n",
    "  - $x_2$ is `ed_years`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_salaries = mu[0]\n",
    "mu_exp = mu[1]\n",
    "mu_ed = mu[2]\n",
    "sig_salaries = sig[0]\n",
    "sig_exp = sig[1]\n",
    "sig_ed = sig[2]\n",
    "\n",
    "bias_std = (-61.3846 + 9.8462*mu_exp + 7.6154*mu_ed - mu_salaries)/sig_salaries\n",
    "weight_std_1 = 9.8462*sig_exp/sig_salaries\n",
    "weight_std_2 = 7.6154*sig_ed/sig_salaries\n",
    "print(\"y = {} + {} x1 + {} x2\".format(bias_std, weight_std_1, weight_std_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The standardized version of the underlying line is:\n",
    "  $$y = 0.05399 + 0.82508 x_1 + 0.20634 x_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses_2x = []\n",
    "Losses_valid_2x = []\n",
    "epoch = 0\n",
    "line_2x = torch.nn.Linear(in_features=2, out_features=1)\n",
    "line_2x.weight.data = torch.tensor([[0.82508, 0.20634]])\n",
    "line_2x.bias.data = torch.tensor([0.05399])\n",
    "fig_2x = graph_steps('Linear regression gradient descent')\n",
    "train_loss_2x = fig.data[0] # use this variable to add train losses to graph\n",
    "valid_loss_2x = fig.data[1] # use this variable to add validation losses to graph\n",
    "\n",
    "valid_2x_pred_y = line_2x(X_valid[:,0:2].float()).squeeze()\n",
    "valid_2x_mse = mse(valid_2x_pred_y, Y_valid)\n",
    "\n",
    "valid_2x_mse.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph the validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "trace_valid_no_reg = go.Scatter( x=torch.tensor(range(len(Losses_valid)))/len(Losses_valid), y=Losses_valid, name='valid_no_reg' )\n",
    "trace_valid_L1 = go.Scatter( x=torch.tensor(range(len(Losses_valid_l1)))/len(Losses_valid_l1), y=Losses_valid_l1, name='valid_L1' )\n",
    "trace_valid_l2 = go.Scatter( x=torch.tensor(range(len(Losses_valid_l2)))/len(Losses_valid_l2), y=Losses_valid_l2, name='valid_L2' )\n",
    "trace_2x = go.Scatter ( x=[0,1], y=[valid_2x_mse.detach().clone(), valid_2x_mse.detach().clone()], name='valid_underlying_line' )\n",
    "fig.add_traces([trace_valid_no_reg, trace_valid_L1, trace_valid_l2, trace_2x])\n",
    "fig.update_xaxes(title=\"Training Steps [proportion]\")\n",
    "fig.update_yaxes(title=\"MSE\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph the coefficients/slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure( data= [\n",
    "    go.Bar(name='no reg', x=df.columns[1:], y=best_line.weight.data.squeeze().tolist()),\n",
    "    go.Bar(name='L2', x=df.columns[1:], y=best_line_l2.weight.data.squeeze().tolist()),\n",
    "    go.Bar(name='L1', x=df.columns[1:], y=best_line_l1.weight.data.squeeze().tolist()),\n",
    "    go.Bar(name='underlying', x=df.columns[1:3], y=line_2x.weight.data.squeeze().tolist()) ])\n",
    "fig.update_layout(barmode='group')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different models on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are going to calculate the MSE with unstandardized values on test data\n",
    "- We are then going to take the square root of the MSE\n",
    "    - This is called the Root Mean Square Error (RMSE)\n",
    "    - this takes it back into the same scale as our original salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE Unregularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_y = best_line(X_test.float()).squeeze()\n",
    "pred_y = unstandardize(test_pred_y, mu[0], sig[0])\n",
    "test_y_unstand = unstandardize(Y_test, mu[0], sig[0])\n",
    "test_no_reg_mse = mse(pred_y, test_y_unstand)\n",
    "test_no_reg_mse.sqrt().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE L1/Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_y = best_line_l1(X_test.float()).squeeze()\n",
    "pred_y = unstandardize(test_pred_y, mu[0], sig[0])\n",
    "test_y_unstand = unstandardize(Y_test, mu[0], sig[0])\n",
    "test_L1_mse = mse(pred_y, test_y_unstand)\n",
    "test_L1_mse.sqrt().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE L2/Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_y = best_line_l2(X_test.float()).squeeze()\n",
    "pred_y = unstandardize(test_pred_y, mu[0], sig[0])\n",
    "test_y_unstand = unstandardize(Y_test, mu[0], sig[0])\n",
    "test_L2_mse = mse(pred_y, test_y_unstand)\n",
    "test_L2_mse.sqrt().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE Underlying Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_y = line_2x(X_test[:,0:2].float()).squeeze()\n",
    "pred_y = unstandardize(test_pred_y, mu[0], sig[0])\n",
    "test_y_unstand = unstandardize(Y_test, mu[0], sig[0])\n",
    "test_2x_mse = mse(pred_y, test_y_unstand)\n",
    "test_2x_mse.sqrt().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
